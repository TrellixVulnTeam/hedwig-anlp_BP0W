{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ftfy\n",
    "import textacy\n",
    "import csv\n",
    "from gensim.utils import simple_preprocess\n",
    "import unicodedata\n",
    "import swifter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "Lots of stuff here is embedding dependant. For instance, you should filter stop words when using word2vec, but you should not do that for glove. Glove also uses specific preprocessing procedure for twitter. Word2vec recommends stripping most punctuation. FastText replaces things like New York with new_york."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASETS=Path('../../hedwig-data/datasets')\n",
    "STOP_WORDS = list(set(stopwords.words('english')))\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "ALPHABET = dict(map(lambda t: (t[1], t[0]),\n",
    "                    enumerate(list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"))))\n",
    "\n",
    "def get_stop_list(df, text_col=1, min_count=10):\n",
    "    word_freq = {}\n",
    "    remove = {}\n",
    "    for text in df[text_col].tolist():\n",
    "        for word in text.split():\n",
    "            word_freq[word] += 1\n",
    "    for k, v in word_freq:\n",
    "        if v < 10:\n",
    "            remove[k]=True\n",
    "        if float(len(word_freq))/float(v)  >= 5.:\n",
    "            remove[k]=True\n",
    "    return remove\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" {} \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"\"] + [re.sub(r\"([A-Z])\",r\" \\1\", hashtag_body, flags=FLAGS)])\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" \"\n",
    "\n",
    "def tweet_preprocessing(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = text.replace(\"@ HASHTAG \", \"#\")\n",
    "    text = text.replace(\"@ USER \", \"<user> \")\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    text = text.replace(\"@ URL\", \"<url>\")\n",
    "    text = text.replace(\" [ URL ] \", \"<url>\")\n",
    "    text = text.replace(\" [ USER ] \", \"<number>\")\n",
    "    text = text.replace(\" [ NUMBER ] \", \"<user>\")\n",
    "    text = text.replace(\" [ HASHTAG ] \", \"<hashtag>\")\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "    \n",
    "    return text.lower()\n",
    "                 \n",
    "def is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cp = ord(char)\n",
    "        if cp == 0 or cp == 0xfffd or is_control(char):\n",
    "            continue\n",
    "        if is_whitespace(char):\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def fix_contractions(text):\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Aa]re|[Cc]ould|[Dd]id|[Dd]oes|[Dd]o|[Hh]ad|[Hh]as|[Hh]ave|[Ii]s|[Mm]ight|[Mm]ust|[Ss]hould|[Ww]ere|[Ww]ould) n't\",\n",
    "        r\"\\1\\2 not\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Hh]e|[Ii]|[Ss]he|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou) 'll\",\n",
    "        r\"\\1\\2 will\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(r\"(\\b)([Tt]here|[Hh]ere) 's\", r\"\\1\\2 is\", text)\n",
    "    text = re.sub(r\"(\\b)([Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou) 're\", r\"\\1\\2 are\", text)\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Ii]|[Ss]hould|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Ww]ould|[Yy]ou) 've\",\n",
    "        r\"\\1\\2 have\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Hh]e|[Ii]|[Ss]he|[Tt]hey|[Ww]e|[Yy]ou) 'd\",\n",
    "        r\"\\1\\2 would\",\n",
    "        text,\n",
    "    )\n",
    "    # non-standard\n",
    "    text = re.sub(r\"(\\b)([Cc]a) n't\", r\"\\1\\2n not\", text)\n",
    "    text = re.sub(r\"(\\b)([Ii]) 'm\", r\"\\1\\2 am\", text)\n",
    "    text = re.sub(r\"(\\b)([Ll]et) 's\", r\"\\1\\2 us\", text)\n",
    "    text = re.sub(r\"(\\b)([Ww]) on't\", r\"\\1\\2ill not\", text)\n",
    "    text = re.sub(r\"(\\b)([Ss]) han't\", r\"\\1\\2hall not\", text)\n",
    "    text = re.sub(r\"(\\b)([Yy])(?: 'all|a 'll)\", r\"\\1\\2ou all\", text)\n",
    "    #####################################################\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Aa]re|[Cc]ould|[Dd]id|[Dd]oes|[Dd]o|[Hh]ad|[Hh]as|[Hh]ave|[Ii]s|[Mm]ight|[Mm]ust|[Ss]hould|[Ww]ere|[Ww]ould) n ' t\",\n",
    "        r\"\\1\\2 not \",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Hh]e|[Ii]|[Ss]he|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou) ' ll \",\n",
    "        r\"\\1\\2 will \",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(r\"(\\b)([Tt]here|[Hh]ere) ' s \", r\"\\1\\2 is\", text)\n",
    "    text = re.sub(r\"(\\b)([Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou) ' re \", r\"\\1\\2 are\", text)\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Ii]|[Ss]hould|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Ww]ould|[Yy]ou) ' ve \",\n",
    "        r\"\\1\\2 have \",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    text = re.sub(\n",
    "        r\"(\\b)([Hh]e|[Ii]|[Ss]he|[Tt]hey|[Ww]e|[Yy]ou) ' d \",\n",
    "        r\"\\1\\2 would \",\n",
    "        text,\n",
    "    )\n",
    "    # non-standard\n",
    "    text = re.sub(r\"(\\b)([Cc]a) n ' t \", r\"\\1\\2n not \", text)\n",
    "    text = re.sub(r\"(\\b)([Ii]) ' m \", r\"\\1\\2 am \", text)\n",
    "    text = re.sub(r\"(\\b)([Ll]et) ' s \", r\"\\1\\2 us \", text)\n",
    "    text = re.sub(r\"(\\b)([Ww]) on ' t \", r\"\\1\\2ill not \", text)\n",
    "    text = re.sub(r\"(\\b)([Ss]ha) n ' t \", r\"\\1\\2ll not \", text)\n",
    "    text = re.sub(r\"(\\b)([Yy])(?: ' all | a ' ll )\", r\"\\1\\2ou all \", text)\n",
    "    text=text.replace(\" 's \", \"'s \").replace(\" ' s \", \"'s \").replace(\" i ' m \", \" i am \")\n",
    "    return text\n",
    "\n",
    "def char_quantize(string, max_length=1000):\n",
    "    \n",
    "    q=[char for char in list(string.lower()) if char in ALPHABET]\n",
    "    if len(q) > max_length:\n",
    "        q=q[:max_length]\n",
    "    return ''.join(q)\n",
    "    \n",
    "def soft_preprocess(df):\n",
    "    df.iloc[:,1]=df.iloc[:,1].swifter.apply(ftfy.fix_text)\n",
    "    df.iloc[:,1]=df.iloc[:,1].swifter.apply(clean_text)\n",
    "    df.iloc[:,1]=df.iloc[:,1].swifter.apply(\n",
    "        lambda x: x.replace(\n",
    "            '\"', \"\").replace(\"\\n\", \" \").replace(\"\\\\\",\"\").replace(\"`\",\"'\").replace(\"& amp ;\", \" and \"))\n",
    "    df.iloc[:,1]=df.iloc[:,1].swifter.apply(fix_contractions)\n",
    "    return df\n",
    "\n",
    "def df_to_hedwig_tsv(df, dsname, outfilename, num_labels_in_col, preprocess, is_twitter_process=True, stop_words=[],\n",
    "                     label_cols=[0], text_col=1):\n",
    "    def to_tsv(outfpath, labels, texts):\n",
    "        with open(outfpath, 'w', newline='') as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "            for label, text in zip(labels, texts):\n",
    "                if (text) > 10:\n",
    "                    writer.writerow([label, text])\n",
    "    df = preprocess(df)\n",
    "    if is_twitter_process:\n",
    "        df.iloc[:,1]=df.iloc[:,1].swifter.apply(tweet_preprocessing)\n",
    "    df.iloc[:,1] = df.iloc[:,1].swifter.apply(lambda x: char_quantize(x))\n",
    "    df.iloc[:,1] = df.iloc[:,1].swifter.apply(lambda text: \" \".join(\n",
    "        [word for word in text.split() if word not in stop_words and len(word) < 15]).strip())\n",
    "    toremove=get_stop_list(df)\n",
    "    df.iloc[:,1] = df.iloc[:,1].swifter.apply(lambda text: \" \".join(\n",
    "        [word for word in text.split() if word not in toremove]).strip())\n",
    "    df.iloc[:,0] = df.swifter.apply(lambda row: ''.join([str(lbl) for lbl in row[label_cols]]), axis=1)\n",
    "    df = df.iloc[:,[0, 1]]\n",
    "    df.iloc[:,0]=df.iloc[:,0].astype('str')\n",
    "    df.iloc[:,0]=df.iloc[:,0].swifter.apply(\n",
    "        lambda x: x if len(x) == num_labels_in_col else ''.join(\n",
    "            ['0' for i in range(num_labels_in_col-len(x))]\n",
    "        )+str(x)\n",
    "    )\n",
    "    dspath=PATH_TO_DATASETS/dsname\n",
    "    outfpath = dspath/outfilename\n",
    "    df = df.sample(frac=1.0)\n",
    "    to_tsv(outfpath, df.iloc[:,0].tolist(), df.iloc[:,1].tolist())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b9c0898a4746e48fb5e5f5200bf494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70baec5f4a64a3e912e546814b4629a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746ffda9adb140bfbc832437e1cb576b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da14457a9e654840bcd493cec0f0b165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b7e63144df40029ed1c836e46facbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f274708642945709a5f56c926dbedfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334bc1160c1e447ba375e818faa9c914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=46578, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-848f2ea7c613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../hedwig-data/datasets/MBTI/dev.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m tdf=df_to_hedwig_tsv(df, dsname=\"MBTI\", outfilename='dev.tsv', num_labels_in_col=4,\n\u001b[0;32m----> 3\u001b[0;31m                      preprocess=soft_preprocess, is_twitter_process=True, stop_words=STOP_WORDS)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-c9e67c7202cb>\u001b[0m in \u001b[0;36mdf_to_hedwig_tsv\u001b[0;34m(df, dsname, outfilename, num_labels_in_col, preprocess, is_twitter_process, stop_words, label_cols, text_col)\u001b[0m\n\u001b[1;32m    192\u001b[0m     df.iloc[:,1] = df.iloc[:,1].swifter.apply(lambda text: \" \".join(\n\u001b[1;32m    193\u001b[0m         [word for word in text.split() if word not in stop_words and len(word) < 15]).strip())\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mtoremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_stop_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     df.iloc[:,1] = df.iloc[:,1].swifter.apply(lambda text: \" \".join(\n\u001b[1;32m    196\u001b[0m         [word for word in text.split() if word not in toremove]).strip())\n",
      "\u001b[0;32m<ipython-input-8-c9e67c7202cb>\u001b[0m in \u001b[0;36mget_stop_list\u001b[0;34m(df, text_col, min_count)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mword_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mremove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mword_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../hedwig-data/datasets/MBTI/dev.csv')\n",
    "tdf=df_to_hedwig_tsv(df, dsname=\"MBTI\", outfilename='dev.tsv', num_labels_in_col=4,\n",
    "                     preprocess=soft_preprocess, is_twitter_process=True, stop_words=STOP_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../hedwig-data/datasets/MBTI/test.csv')\n",
    "tdf=df_to_hedwig_tsv(df, dsname=\"MBTI\", outfilename='test.tsv', num_labels_in_col=4,\n",
    "                     preprocess=soft_preprocess, is_twitter_process=True, stop_words=STOP_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../hedwig-data/datasets/MBTI/train.csv')\n",
    "tdf=df_to_hedwig_tsv(df, dsname=\"MBTI\", outfilename='train.tsv', num_labels_in_col=4,\n",
    "                     preprocess=soft_preprocess, is_twitter_process=True, stop_words=STOP_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
